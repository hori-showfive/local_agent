# 目的

ローカル LLM が仮想環境を自由に操作して様々なタスクを自律的にこなせるような AI エージェントシステムを作りたい。
AI の推論環境と AI が自由にコマンド操作を実行できる環境を docker を使用して構築したい。
"docker コンテナ内で全てが完結するシステムを作りたい。"


# 実装戦略
- 最もシンプルで最小限の機能から少しずつ拡張して作り上げていく
- テスト駆動開発

# 技術仕様
- ollama サーバを使用して[deepcoder](https://ollama.com/library/deepcoder:14b)を実行
- ollama docker image をベースに環境を作っていく
- LLM とシェルの仲介システムは nodejs
- LLM への指示やアクティビティのモニタリングは nextjs+typescript でフロントエンドサーバを作りホストからブラウザでアクセスできるようにする
- docker コンテナ内部で ollama もバックエンドサーバもフロントエンドサーバも起動する。ホストから完全に独立して機能を提供する。

# 大まかな実装計画
- [x] まず ollama docker image で[deepcoder](https://ollama.com/library/deepcoder:14b)が実行できることを確認する
- [x] 次に、nodejs で ollama サーバにリクエストを飛ばして結果を受け取る最もシンプルなプログラムを書いて、動作確認する
- [x] nodejs でシェルコマンドを実行する最小限のプログラムを書く
- [x] hello world を表示する最小限のフロントエンドサーバを構築する
- [x] nodejs でシェルコマンドを実行する最小限のプログラムを拡張して、バックエンドサーバとして機能するように最低限の API を実装する。最低限の API とは、POST リクエストで LLM への指示を受け取って、ollamaAPI をたたいてその指示を送信、ollama からのレスポンスをクライアントに送信する API
- [ ] dockerでそれぞれのサーバが立ち上がって正常に動作しているのか検証する。(デバッグ作業)
- [ ] ソースコードをリファクタリングする。
- [ ] AI エージェントとして機能させるための詳しい仕様を考える

# その他

- 開発環境は windows11,powershell,vscode,cuda ツールキットはインストール済み、gpu は nvidia 4070 Ti Super
